{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.google/tools/datasets/google-facial-expression/\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import pickle\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('FEC_dataset/two_class_ready.csv')\n",
    "train_df.head()\n",
    "# hyperparams\n",
    "SHAPE=(160,160,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136379 triplets in the dataset\n",
      "Before loop\n",
      "====================================\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
     ]
    }
   ],
   "source": [
    "def TripletGen(data, root='FEC_dataset/formatted', batch_size=128, rescale=1./255, shape=SHAPE[:2]):\n",
    "    # yielding format must be [anchorlist, positlist, negatlist], dummy\n",
    "    # 3 elements of shape (@batch_size,*@shape) in the list and an empty np.array of shape(@batch_size,1)\n",
    "    \n",
    "    ## the init code\n",
    "    n = len(data)\n",
    "    iterations = math.ceil(n/batch_size)\n",
    "    print(f\"Found {n} triplets in the dataset\")\n",
    "    \n",
    "    def genFunc(batch_size = batch_size):\n",
    "        y_dummy = np.empty((batch_size,))\n",
    "        while True:\n",
    "            for i in range(iterations):\n",
    "                # use start and end as indices to work with\n",
    "                start, end = i*batch_size, min((i+1)*batch_size, n)\n",
    "                batch_size_eff = end - start\n",
    "\n",
    "                arr_shape = (batch_size_eff, *shape, 3)\n",
    "                anchor = np.empty(arr_shape)\n",
    "                positive = np.empty(arr_shape)\n",
    "                negative = np.empty(arr_shape)\n",
    "\n",
    "                for arr_index in range(batch_size_eff):\n",
    "                    data_index = arr_index + start\n",
    "\n",
    "                    # anchor\n",
    "                    path = os.path.join(root, data[\"Image1\"][data_index])\n",
    "                    temp = image.load_img( path, target_size = shape )\n",
    "                    anchor[arr_index] = image.img_to_array(temp)*rescale\n",
    "\n",
    "                    # positive\n",
    "                    path = os.path.join(root, data[\"Image2\"][data_index])\n",
    "                    temp = image.load_img( path, target_size = shape )\n",
    "                    positive[arr_index] = image.img_to_array(temp)*rescale\n",
    "\n",
    "                    # negative\n",
    "                    path = os.path.join(root, data[\"Image3\"][data_index])\n",
    "                    temp = image.load_img( path, target_size = shape )\n",
    "                    negative[arr_index] = image.img_to_array(temp)*rescale\n",
    "\n",
    "                yield [anchor, positive, negative], y_dummy[:batch_size_eff]\n",
    "    return genFunc(), iterations\n",
    "\n",
    "imgs, iterations = TripletGen(train_df)\n",
    "print(\"Before loop\")\n",
    "print(\"==\"*18)\n",
    "\n",
    "for i in range(5):\n",
    "    left, right = next(imgs)\n",
    "    for i, thing in enumerate(left):\n",
    "        print(f\"{i+1}th element: \",thing.shape)\n",
    "    print(\"dummy\", right.shape)\n",
    "    print(\"=-\"*18)\n",
    "    \n",
    "del(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tf_triplet_loss(y_true, y_pred, alpha=0.4):\n",
    "    n = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:,0:n//3]\n",
    "    positive = y_pred[:,n//3:2*n//3]\n",
    "    negative = y_pred[:,2*n//3:n]\n",
    "    \n",
    "    sigma_pd = tf.math.reduce_sum(tf.norm(positive-anchor, axis=1))\n",
    "    sigma_nd = tf.math.reduce_sum(tf.norm(negative-anchor, axis=1))\n",
    "\n",
    "    loss = sigma_nd - sigma_pd + alpha\n",
    "    return tf.math.log(1.0+tf.math.exp(loss))\n",
    "#     return tf.math.maximum(loss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0907 20:50:16.701715 139631116175168 deprecation.py:506] From /home/deepak/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 16)           704688      anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 48)           0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "                                                                 sequential[3][0]                 \n",
      "==================================================================================================\n",
      "Total params: 704,688\n",
      "Trainable params: 704,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2 = tf.keras.regularizers.l2\n",
    "alpha = 0\n",
    "base_model = Sequential([\n",
    "    #first convolution\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=SHAPE),#, activity_regularizer=l2(alpha)),\n",
    "    MaxPool2D(2,2),\n",
    "    #second\n",
    "    Conv2D(32, (3,3), activation='relu'),#, activity_regularizer=l2(alpha)),\n",
    "    MaxPool2D(2,2),\n",
    "    #third\n",
    "    Conv2D(64, (3,3), activation='relu'),#, activity_regularizer=l2(alpha)),\n",
    "    MaxPool2D(2,2),\n",
    "    #fourth\n",
    "    Conv2D(64, (3,3), activation='relu'),#, activity_regularizer=l2(alpha)),\n",
    "    MaxPool2D(2,2),\n",
    "    #fifth\n",
    "    Conv2D(64, (3,3), activation='relu'),#, activity_regularizer=l2(alpha)),\n",
    "    MaxPool2D(2,2),\n",
    "    # hidden\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='softplus'),#, activity_regularizer=l2(alpha)),\n",
    "    #Last layer\n",
    "    Dense(16, activation='tanh')\n",
    "])\n",
    "anchor_input = Input(SHAPE, name='anchor_input')\n",
    "positive_input = Input(SHAPE, name='positive_input')\n",
    "negative_input = Input(SHAPE, name='negative_input')\n",
    "\n",
    "encoded_anchor = base_model(anchor_input)\n",
    "encoded_positive = base_model(positive_input)\n",
    "encoded_negative = base_model(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "train_model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136379 triplets in the dataset\n"
     ]
    }
   ],
   "source": [
    "mySGD = SGD(nesterov=True)\n",
    "train_model.compile(loss=batch_tf_triplet_loss, optimizer=mySGD)\n",
    "train, steps = TripletGen(train_df, batch_size=128)\n",
    "checkpoint = ModelCheckpoint(\"emotion.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to get a taste of the model\n",
    "# 1.0238742\n",
    "# m, b = 120, 120\n",
    "# dgen, dsteps = TripletGen(train_df[:m], batch_size=b)\n",
    "# x = []\n",
    "# for i, (images, dy) in tqdm(enumerate(dgen)):\n",
    "#     if i< math.ceil(m//b):\n",
    "#         x.append(train_model.predict(images))\n",
    "#     else: \n",
    "#         break\n",
    "# print(len(x))\n",
    "# loss = tf.Variable(tf.zeros([1]))\n",
    "# for i, num in enumerate(x):\n",
    "#     loss = loss + batch_tf_triplet_loss(None, tf.convert_to_tensor(num, np.float32))\n",
    "# print(\"before sess\")\n",
    "# with tf.Session() as sess:\n",
    "#     init_op = tf.global_variables_initializer()\n",
    "#     sess.run(init_op)\n",
    "#     with sess.as_default():\n",
    "#         print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00001: loss did not improve from 0.91298\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9131\n",
      "Epoch 2/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9140\n",
      "Epoch 00002: loss did not improve from 0.91298\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9140\n",
      "Epoch 3/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9138\n",
      "Epoch 00003: loss did not improve from 0.91298\n",
      "1066/1066 [==============================] - 763s 716ms/step - loss: 0.9138\n",
      "Epoch 4/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00004: loss did not improve from 0.91298\n",
      "1066/1066 [==============================] - 759s 712ms/step - loss: 0.9131\n",
      "Epoch 5/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00005: loss did not improve from 0.91298\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9130\n",
      "Epoch 6/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00006: loss improved from 0.91298 to 0.91295, saving model to emotion.hdf5\n",
      "1066/1066 [==============================] - 761s 714ms/step - loss: 0.9129\n",
      "Epoch 7/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00007: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 762s 715ms/step - loss: 0.9130\n",
      "Epoch 8/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9136\n",
      "Epoch 00008: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9136\n",
      "Epoch 9/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9137\n",
      "Epoch 00009: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9137\n",
      "Epoch 10/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00010: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 760s 713ms/step - loss: 0.9130\n",
      "Epoch 11/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9151\n",
      "Epoch 00011: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 761s 714ms/step - loss: 0.9151\n",
      "Epoch 12/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9142\n",
      "Epoch 00012: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 757s 710ms/step - loss: 0.9142\n",
      "Epoch 13/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00013: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 761s 714ms/step - loss: 0.9130\n",
      "Epoch 14/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00014: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 758s 711ms/step - loss: 0.9130\n",
      "Epoch 15/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00015: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 759s 712ms/step - loss: 0.9130\n",
      "Epoch 16/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9132\n",
      "Epoch 00016: loss did not improve from 0.91295\n",
      "1066/1066 [==============================] - 761s 714ms/step - loss: 0.9132\n",
      "Epoch 17/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9123\n",
      "Epoch 00017: loss improved from 0.91295 to 0.91233, saving model to emotion.hdf5\n",
      "1066/1066 [==============================] - 717s 673ms/step - loss: 0.9123\n",
      "Epoch 18/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9124\n",
      "Epoch 00018: loss did not improve from 0.91233\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9124\n",
      "Epoch 19/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9121\n",
      "Epoch 00019: loss improved from 0.91233 to 0.91206, saving model to emotion.hdf5\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9121\n",
      "Epoch 20/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9113\n",
      "Epoch 00020: loss improved from 0.91206 to 0.91129, saving model to emotion.hdf5\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9113\n",
      "Epoch 21/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00021: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9131\n",
      "Epoch 22/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00022: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 23/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00023: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 687s 645ms/step - loss: 0.9131\n",
      "Epoch 24/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9133\n",
      "Epoch 00024: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9133\n",
      "Epoch 25/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00025: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 26/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9129\n",
      "Epoch 00026: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9129\n",
      "Epoch 27/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00027: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 683s 641ms/step - loss: 0.9130\n",
      "Epoch 28/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00028: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 29/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00029: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 30/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9128\n",
      "Epoch 00030: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9128\n",
      "Epoch 31/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00031: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9131\n",
      "Epoch 32/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9124\n",
      "Epoch 00032: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9124\n",
      "Epoch 33/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9133\n",
      "Epoch 00033: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9133\n",
      "Epoch 34/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9129\n",
      "Epoch 00034: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9129\n",
      "Epoch 35/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9128\n",
      "Epoch 00035: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9128\n",
      "Epoch 36/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9129\n",
      "Epoch 00036: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9129\n",
      "Epoch 37/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9141\n",
      "Epoch 00037: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9141\n",
      "Epoch 38/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00038: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 39/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9132\n",
      "Epoch 00039: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9132\n",
      "Epoch 40/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00040: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00041: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9131\n",
      "Epoch 42/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00042: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 43/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00043: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 44/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00044: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 45/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00045: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 46/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00046: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 47/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9132\n",
      "Epoch 00047: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9132\n",
      "Epoch 48/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00048: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9131\n",
      "Epoch 49/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00049: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 50/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00050: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 51/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00051: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 52/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00052: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 53/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00053: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 54/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00054: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 55/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00055: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 56/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00056: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 688s 645ms/step - loss: 0.9130\n",
      "Epoch 57/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00057: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9131\n",
      "Epoch 58/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00058: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 59/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00059: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 60/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00060: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9131\n",
      "Epoch 61/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9133\n",
      "Epoch 00061: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9133\n",
      "Epoch 62/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9131\n",
      "Epoch 00062: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9131\n",
      "Epoch 63/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00063: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 641ms/step - loss: 0.9130\n",
      "Epoch 64/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00064: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 65/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00065: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 684s 642ms/step - loss: 0.9130\n",
      "Epoch 66/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00066: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 689s 647ms/step - loss: 0.9130\n",
      "Epoch 67/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00067: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 686s 643ms/step - loss: 0.9130\n",
      "Epoch 68/100\n",
      "1065/1066 [============================>.] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00068: loss did not improve from 0.91129\n",
      "1066/1066 [==============================] - 689s 646ms/step - loss: 0.9130\n",
      "Epoch 69/100\n",
      " 500/1066 [=============>................] - ETA: 6:20 - loss: 0.9130"
     ]
    }
   ],
   "source": [
    "history = train_model.fit_generator(train, epochs=100, steps_per_epoch=steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efde93ecdd8>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.kill(os.getpid(), signal.SIGKILL)\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")\n",
    "seq = train_model.layers[3]\n",
    "seq.layers[6]#.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[anchor_input], outputs=encoded_anchor)\n",
    "model.load_weights(\"model.h5\")\n",
    "print(\"Loaded weights from h5 file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, steps = TripletGen(train_df[:1], batch_size=1)\n",
    "images, _ = next(train)\n",
    "anchor, positive, negative = tuple(images)\n",
    "print(anchor.shape)\n",
    "# a,p,n = train_model.predict(anchor), model.predict(positive), model.predict(negative)\n",
    "train_model.predict(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
