{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.google/tools/datasets/google-facial-expression/\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('FEC_dataset/two_class_ready.csv')\n",
    "train_df.head()\n",
    "# hyperparams\n",
    "SHAPE=(160,160,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 136379 triplets in the dataset\n",
      "Before loop\n",
      "====================================\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
      "1th element:  (128, 160, 160, 3)\n",
      "2th element:  (128, 160, 160, 3)\n",
      "3th element:  (128, 160, 160, 3)\n",
      "dummy (128,)\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n"
     ]
    }
   ],
   "source": [
    "def TripletGen(data, root='FEC_dataset/formatted', batch_size=128, rescale=1./255, shape=SHAPE[:2]):\n",
    "    # yielding format must be [anchorlist, positlist, negatlist], dummy\n",
    "    # 3 elements of shape (@batch_size,*@shape) in the list and an empty np.array of shape(@batch_size,1)\n",
    "    \n",
    "    ## the init code\n",
    "    n = len(data)\n",
    "    iterations = math.ceil(n/batch_size)\n",
    "    print(f\"Found {n} triplets in the dataset\")\n",
    "    \n",
    "    def genFunc(batch_size = batch_size):\n",
    "        y_dummy = np.empty((batch_size,))\n",
    "        for i in range(iterations):\n",
    "            # use start and end as indices to work with\n",
    "            start, end = i*batch_size, min((i+1)*batch_size, n)\n",
    "            batch_size = end - start\n",
    "            \n",
    "            arr_shape = (batch_size, *shape, 3)\n",
    "            anchor = np.empty(arr_shape)\n",
    "            positive = np.empty(arr_shape)\n",
    "            negative = np.empty(arr_shape)\n",
    "            \n",
    "            for arr_index in range(batch_size):\n",
    "                data_index = arr_index + start\n",
    "                \n",
    "                # anchor\n",
    "                path = os.path.join(root, data[\"Image1\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                anchor[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "                # positive\n",
    "                path = os.path.join(root, data[\"Image2\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                positive[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "                # negative\n",
    "                path = os.path.join(root, data[\"Image3\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                negative[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "            yield [anchor, positive, negative], y_dummy\n",
    "    return genFunc(), iterations\n",
    "\n",
    "imgs, iterations = TripletGen(train_df)\n",
    "print(\"Before loop\")\n",
    "print(\"==\"*18)\n",
    "\n",
    "for i in range(5):\n",
    "    left, right = next(imgs)\n",
    "    for i, thing in enumerate(left):\n",
    "        print(f\"{i+1}th element: \",thing.shape)\n",
    "    print(\"dummy\", right.shape)\n",
    "    print(\"=-\"*18)\n",
    "    \n",
    "del(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tf_triplet_loss(y_true, y_pred, alpha=0.4):\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "    \n",
    "    sigma_pd = tf.math.reduce_sum(tf.norm(positive-anchor, axis=1))\n",
    "    sigma_nd = tf.math.reduce_sum(tf.norm(negative-anchor, axis=1))\n",
    "    loss = sigma_nd - sigma_pd + alpha\n",
    "    return tf.maximum(loss, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 19:05:01.176491 140166284699456 deprecation.py:506] From /home/deepak/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 158, 158, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 79, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 77, 77, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 38, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 36, 36, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              590848    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                16400     \n",
      "=================================================================\n",
      "Total params: 704,688\n",
      "Trainable params: 704,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = Sequential([\n",
    "    #first convolution\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=SHAPE),\n",
    "    MaxPool2D(2,2),\n",
    "    #second\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #third\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #fourth\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #fifth\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    # hidden\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    #Last layer\n",
    "    Dense(16)\n",
    "])\n",
    "base_model.summary()\n",
    "anchor_input = Input(SHAPE, name='anchor_input')\n",
    "positive_input = Input(SHAPE, name='positive_input')\n",
    "negative_input = Input(SHAPE, name='negative_input')\n",
    "\n",
    "encoded_anchor = base_model(anchor_input)\n",
    "encoded_positive = base_model(positive_input)\n",
    "encoded_negative = base_model(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "train_model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_tf_triplet_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTripletGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emotion.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model.compile(loss=batch_tf_triplet_loss, optimizer=Adam())\n",
    "train, steps = TripletGen(train_df, batch_size=80)\n",
    "checkpoint = ModelCheckpoint(\"emotion.hdf5\", monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model.fit_generator(train, epochs=20, steps_per_epoch=steps, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
