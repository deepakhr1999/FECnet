{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://ai.google/tools/datasets/google-facial-expression/\n",
    "import imageio\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import * \n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('FEC_dataset/two_class_ready.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TripletGen(data, root='FEC_dataset/formatted', batch_size=128, rescale=1./255, shape=(224,224)):\n",
    "    # yielding format must be [anchorlist, positlist, negatlist], dummy\n",
    "    # 3 elements of shape (@batch_size,*@shape) in the list and an empty np.array of shape(@batch_size,1)\n",
    "    \n",
    "    ## the init code\n",
    "    n = len(data)\n",
    "    iterations = math.ceil(n/batch_size)\n",
    "    print(f\"Found {n} triplets in the dataset\")\n",
    "    \n",
    "    def genFunc(batch_size = batch_size):\n",
    "        y_dummy = np.empty((batch_size,))\n",
    "        for i in range(iterations):\n",
    "            # use start and end as indices to work with\n",
    "            start, end = i*batch_size, min((i+1)*batch_size, n)\n",
    "            batch_size = end - start\n",
    "            \n",
    "            arr_shape = (batch_size, *shape, 3)\n",
    "            anchor = np.empty(arr_shape)\n",
    "            positive = np.empty(arr_shape)\n",
    "            negative = np.empty(arr_shape)\n",
    "            \n",
    "            for arr_index in range(batch_size):\n",
    "                data_index = arr_index + start\n",
    "                \n",
    "                # anchor\n",
    "                path = os.path.join(root, data[\"Image1\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                anchor[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "                # positive\n",
    "                path = os.path.join(root, data[\"Image2\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                positive[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "                # negative\n",
    "                path = os.path.join(root, data[\"Image3\"][data_index])\n",
    "                temp = image.load_img( path, target_size = shape )\n",
    "                negative[arr_index] = image.img_to_array(temp)*rescale\n",
    "                \n",
    "            yield [anchor, positive, negative], y_dummy\n",
    "    return genFunc()\n",
    "\n",
    "imgs = TripletGen(train_df)\n",
    "print(\"Before loop\")\n",
    "print(\"==\"*18)\n",
    "\n",
    "for i in range(5):\n",
    "    left, right = next(imgs)\n",
    "    for i, thing in enumerate(left):\n",
    "        print(f\"{i+1}th element: \",thing.shape)\n",
    "    print(\"dummy\", right.shape)\n",
    "    print(\"=-\"*18)\n",
    "    \n",
    "del(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tf_triplet_loss(y_true, y_pred, alpha=0.4):\n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "    \n",
    "    sigma_pd = tf.math.reduce_sum(tf.norm(positive-anchor, axis=1))\n",
    "    sigma_nd = tf.math.reduce_sum(tf.norm(negative-anchor, axis=1))\n",
    "    loss = sigma_nd - sigma_pd + alpha\n",
    "    return tf.maximum(loss, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (224,224,3)\n",
    "\n",
    "base_model = Sequential([\n",
    "    #first convolution\n",
    "    Conv2D(16, (3,3), activation='relu', input_shape=shape),\n",
    "    MaxPool2D(2,2),\n",
    "    #second\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #third\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #fourth\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    #fifth\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPool2D(2,2),\n",
    "    # hidden\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    #Last layer\n",
    "    Dense(16)\n",
    "])\n",
    "\n",
    "anchor_input = Input(shape, name='anchor_input')\n",
    "positive_input = Input(shape, name='positive_input')\n",
    "negative_input = Input(shape, name='negative_input')\n",
    "\n",
    "encoded_anchor = base_model(anchor_input)\n",
    "encoded_positive = base_model(positive_input)\n",
    "encoded_negative = base_model(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "train_model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.compile(loss=batch_tf_triplet_loss, optimizer=Adam())\n",
    "train = TripletGen(train_df, batch_size=80)\n",
    "history = train_model.fit_generator(train, epochs=20, steps_per_epoch=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
